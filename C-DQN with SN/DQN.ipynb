{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbb71b74",
   "metadata": {},
   "source": [
    "# LunarLander-v2 solver\n",
    "## 1. Functions and classes\n",
    "Import packages, network class setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7160e239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym.spaces as sp\n",
    "from tqdm import trange\n",
    "from time import sleep\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from OpenGL.GL import glPushMatrix\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "print(device)\n",
    "\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, n_states, n_actions, n_hidden=64):\n",
    "        super(QNet, self).__init__()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_states, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_actions)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "#%% dqn    \n",
    "class DQN():\n",
    "    def __init__(self, n_states, n_actions, batch_size=64, lr=1e-4, gamma=0.99, mem_size=int(1e5), learn_step=5, tau=1e-3):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.learn_step = learn_step    # for very learn_step steps, update once\n",
    "        self.tau = tau\n",
    "\n",
    "        # model\n",
    "        self.net_eval = QNet(n_states, n_actions).to(device)\n",
    "        self.net_target = QNet(n_states, n_actions).to(device)\n",
    "        self.optimizer = optim.Adam(self.net_eval.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        # memory\n",
    "        self.memory = ReplayBuffer(n_actions, mem_size, batch_size)\n",
    "        self.counter = 0    # update cycle counter\n",
    "\n",
    "    def getAction(self, state, epsilon):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "\n",
    "        self.net_eval.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.net_eval(state)\n",
    "        self.net_eval.train()\n",
    "\n",
    "        # epsilon-greedy\n",
    "        if random.random() < epsilon:\n",
    "            action = random.choice(np.arange(self.n_actions))\n",
    "        else:\n",
    "            action = np.argmax(action_values.cpu().data.numpy())\n",
    "\n",
    "        return action\n",
    "\n",
    "    def save2memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        self.counter += 1\n",
    "        if self.counter % self.learn_step == 0:\n",
    "            if len(self.memory) >= self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences)\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        q_target = self.net_target(next_states).detach().max(axis=1)[0].unsqueeze(1)\n",
    "        y_j = rewards + self.gamma * q_target * (1 - dones)          # target, if terminal then y_j = rewards\n",
    "        q_eval = self.net_eval(states).gather(1, actions)\n",
    "\n",
    "        # loss backprop\n",
    "        loss = self.criterion(q_eval, y_j)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # soft update target network\n",
    "        self.softUpdate()\n",
    "\n",
    "    def softUpdate(self):\n",
    "        for eval_param, target_param in zip(self.net_eval.parameters(), self.net_target.parameters()):\n",
    "            target_param.data.copy_(self.tau*eval_param.data + (1.0-self.tau)*target_param.data)\n",
    "\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, n_actions, memory_size, batch_size):\n",
    "        self.n_actions = n_actions\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen = memory_size)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93a1e3e9",
   "metadata": {},
   "source": [
    "Traning and Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f40bd905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, n_episodes=2000, max_steps=1000, eps_start=1.0, eps_end=0.1, eps_decay=0.995, target=200, chkpt=False):\n",
    "    score_hist = []\n",
    "    epsilon = eps_start\n",
    "\n",
    "    bar_format = '{l_bar}{bar:10}| {n:4}/{total_fmt} [{elapsed:>7}<{remaining:>7}, {rate_fmt}{postfix}]'\n",
    "    pbar = trange(n_episodes, unit=\"ep\", bar_format=bar_format, ascii=True)\n",
    "    for idx_epi in pbar:\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for idx_step in range(max_steps):\n",
    "            action = agent.getAction(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.save2memory(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        score_hist.append(score)\n",
    "        score_avg = np.mean(score_hist[-100:])\n",
    "        epsilon = max(eps_end, epsilon*eps_decay)\n",
    "\n",
    "        pbar.set_postfix_str(f\"Score: {score: 7.2f}, 100 score avg: {score_avg: 7.2f}\")\n",
    "        pbar.update(0)\n",
    "\n",
    "        if len(score_hist) >= 100:\n",
    "            if score_avg >= target:\n",
    "                break\n",
    "\n",
    "    if (idx_epi+1) < n_episodes:\n",
    "        print(\"\\nTarget Reached!\")\n",
    "    else:\n",
    "        print(\"\\nDone!\")\n",
    "\n",
    "    if chkpt:\n",
    "        torch.save(agent.net_eval.state_dict(), 'checkpoint.pth')\n",
    "\n",
    "    # Save score_hist to a text file\n",
    "    with open('DQN_sc_hist.txt', 'w') as file:\n",
    "        for score in score_hist:\n",
    "            file.write(f\"{score}\\n\")\n",
    "\n",
    "    return score_hist\n",
    "\n",
    "#%% Test Lunar Lander\n",
    "def testLander(env, agent, loop=3):\n",
    "    for i in range(loop):\n",
    "        state = env.reset()\n",
    "        for idx_step in range(500):\n",
    "            action = agent.getAction(state, epsilon=0)\n",
    "            env.render()\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "    env.close()\n",
    "    \n",
    "def plotScore(scores):\n",
    "    plt.figure()\n",
    "    plt.plot(scores)\n",
    "    plt.title(\"Score History\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94829961",
   "metadata": {},
   "source": [
    "## 2. Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57a9abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "EPISODES = 2000\n",
    "TARGET_SCORE = 250.     # early training stop at avg score of last 100 episodes\n",
    "GAMMA = 0.99            # discount factor\n",
    "MEMORY_SIZE = 10000     # max memory buffer size\n",
    "LEARN_STEP = 5          # how often to learn\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "SAVE_CHKPT = False      # save trained network .pth file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfd2ffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "agent = DQN(\n",
    "    n_states = num_states,\n",
    "    n_actions = num_actions,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    lr = LR,\n",
    "    gamma = GAMMA,\n",
    "    mem_size = MEMORY_SIZE,\n",
    "    learn_step = LEARN_STEP,\n",
    "    tau = TAU,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02799e76",
   "metadata": {},
   "source": [
    "## 3. Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "407052f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|2         |   58/2000 [  00:08<  05:00,  6.47ep/s, Score:  -71.13, 100 score avg: -168.05]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/srinath/Documents/C-DQN with SN/DQN.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/DQN.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(device) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/DQN.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# torch.cuda.empty_cache()\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/DQN.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDevice: CUDA\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/DQN.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m score_hist \u001b[39m=\u001b[39m train(env, agent, n_episodes\u001b[39m=\u001b[39;49mEPISODES, target\u001b[39m=\u001b[39;49mTARGET_SCORE, chkpt\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/DQN.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m plotScore(score_hist)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/DQN.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(device) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;32m/home/srinath/Documents/C-DQN with SN/DQN.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/DQN.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mgetAction(state, epsilon)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/DQN.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m next_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/DQN.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m agent\u001b[39m.\u001b[39;49msave2memory(state, action, reward, next_state, done)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/DQN.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m state \u001b[39m=\u001b[39m next_state\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/DQN.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n",
      "\u001b[1;32m/home/srinath/Documents/C-DQN with SN/DQN.ipynb Cell 9\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/DQN.ipynb#X11sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcounter \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearn_step \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/DQN.ipynb#X11sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/DQN.ipynb#X11sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m         experiences \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory\u001b[39m.\u001b[39;49msample()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/DQN.ipynb#X11sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearn(experiences)\n",
      "\u001b[1;32m/home/srinath/Documents/C-DQN with SN/DQN.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/DQN.ipynb#X11sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msample\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/DQN.ipynb#X11sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m     experiences \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39msample(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory, k\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/DQN.ipynb#X11sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m     states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39;49mvstack([e\u001b[39m.\u001b[39;49mstate \u001b[39mfor\u001b[39;49;00m e \u001b[39min\u001b[39;49;00m experiences \u001b[39mif\u001b[39;49;00m e \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m]))\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/DQN.ipynb#X11sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m     actions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39mvstack([e\u001b[39m.\u001b[39maction \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m experiences \u001b[39mif\u001b[39;00m e \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]))\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/DQN.ipynb#X11sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m     rewards \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39mvstack([e\u001b[39m.\u001b[39mreward \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m experiences \u001b[39mif\u001b[39;00m e \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]))\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/shape_base.py:296\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(arrs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    295\u001b[0m     arrs \u001b[39m=\u001b[39m [arrs]\n\u001b[0;32m--> 296\u001b[0m \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(arrs, \u001b[39m0\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mdtype, casting\u001b[39m=\u001b[39;49mcasting)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if str(device) == \"cuda\":\n",
    "    # torch.cuda.empty_cache()\n",
    "    print(\"Device: CUDA\")\n",
    "score_hist = train(env, agent, n_episodes=EPISODES, target=TARGET_SCORE, chkpt=True)\n",
    "\n",
    "plotScore(score_hist)\n",
    "\n",
    "if str(device) == \"cuda\":\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d78ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(agent.net_eval.state_dict(), 'checkpoint_2k_eps.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c47764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model from checkpoint.pth\n",
    "# agent.net_eval.load_state_dict(torch.load('./checkpoint_jun30_1k_iter.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'score_hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/srinath/Documents/DQN-for-LunarLander-v2-main/LunarLander.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/srinath/Documents/DQN-for-LunarLander-v2-main/LunarLander.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m plotScore(score_hist)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'score_hist' is not defined"
     ]
    }
   ],
   "source": [
    "plotScore(score_hist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9cb59b1",
   "metadata": {},
   "source": [
    "## 4. Test the LunarLander!\n",
    "Run code below to test trained result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b48127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testLander(env, agent, loop=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be4be6b2",
   "metadata": {},
   "source": [
    "Run code below to save tested results into a gif!   \n",
    "Saved gifs will appear in `gifs` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03247c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving gif...Done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import imageio\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def TextOnImg(img, score):\n",
    "    img = Image.fromarray(img)\n",
    "    font = ImageFont.truetype('./arial.ttf', 18)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw.text((20, 20), f\"Score={score: .2f}\", font=font, fill=(255, 255, 255))\n",
    "\n",
    "    return np.array(img)\n",
    "\n",
    "def save_frames_as_gif(frames, filename, path=\"gifs/\"):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    print(\"Saving gif...\", end=\"\")\n",
    "    imageio.mimsave(path + filename + \".gif\", frames, duration=60)\n",
    "\n",
    "    print(\"Done!\")\n",
    "\n",
    "def gym2gif(env, agent, filename=\"gym_animation_dqn\", loop=1):\n",
    "    frames = []\n",
    "    for i in range(loop):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for idx_step in range(500):\n",
    "            frame = env.render(mode=\"rgb_array\")\n",
    "            frames.append(TextOnImg(frame, score))\n",
    "            action = agent.getAction(state, epsilon=0)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "    env.close()\n",
    "    save_frames_as_gif(frames, filename=filename)\n",
    "\n",
    "gym2gif(env, agent, loop=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550880bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "01c36111a1525a5ff407868170e7863364c687246ea4e433c1c4d9e429760133"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
