{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbb71b74",
   "metadata": {},
   "source": [
    "# LunarLander-v2 solver\n",
    "## 1. Functions and classes\n",
    "Import packages, network class setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7160e239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.spectral_norm as spectral_norm\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym.spaces as sp\n",
    "from tqdm import trange\n",
    "from time import sleep\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from OpenGL.GL import glPushMatrix\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "print(device)\n",
    "\n",
    "class QNet(nn.Module):\n",
    "   def __init__(self, n_states, n_actions, n_hidden=64):\n",
    "       super(QNet, self).__init__()\n",
    "\n",
    "       self.fc = nn.Sequential(\n",
    "           nn.Linear(n_states, n_hidden),\n",
    "           nn.ReLU(),\n",
    "           nn.Linear(n_hidden, n_hidden),\n",
    "           nn.ReLU(),\n",
    "           spectral_norm(nn.Linear(n_hidden, n_actions)) # Apply spectral normalization to the last layer\n",
    "       )\n",
    "\n",
    "   def forward(self, x):\n",
    "       return self.fc(x)\n",
    "   \n",
    "# class QNet(nn.Module):\n",
    "#     def __init__(self, n_states, n_actions, n_hidden=64):\n",
    "#         super(QNet, self).__init__()\n",
    "\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(n_states, n_hidden),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(n_hidden, n_hidden),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(n_hidden, n_actions),\n",
    "#             nn.Linear(n_hidden, n_actions),\n",
    "#             spectral_norm(nn.Linear(n_hidden, n_actions))\n",
    "#             )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.fc(x)\n",
    " \n",
    "#%% dqn    \n",
    "class DQN():\n",
    "    def __init__(self, n_states, n_actions, batch_size=64, lr=1e-4, gamma=0.99, mem_size=int(1e5), learn_step=5, tau=1e-3):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.learn_step = learn_step    # for very learn_step steps, update once\n",
    "        self.tau = tau\n",
    "\n",
    "        # model\n",
    "        self.net_eval = QNet(n_states, n_actions).to(device)\n",
    "        self.net_target = QNet(n_states, n_actions).to(device)\n",
    "        self.optimizer = optim.Adam(self.net_eval.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        # memory\n",
    "        self.memory = ReplayBuffer(n_actions, mem_size, batch_size)\n",
    "        self.counter = 0    # update cycle counter\n",
    "\n",
    "    def getAction(self, state, epsilon):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "\n",
    "        self.net_eval.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.net_eval(state)\n",
    "        self.net_eval.train()\n",
    "\n",
    "        # epsilon-greedy\n",
    "        if random.random() < epsilon:\n",
    "            action = random.choice(np.arange(self.n_actions))\n",
    "        else:\n",
    "            action = np.argmax(action_values.cpu().data.numpy())\n",
    "\n",
    "        return action\n",
    "\n",
    "    def save2memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        self.counter += 1\n",
    "        if self.counter % self.learn_step == 0:\n",
    "            if len(self.memory) >= self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences)\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        q_target = self.net_target(next_states).detach().max(axis=1)[0].unsqueeze(1)\n",
    "        q_self = self.net_eval(next_states).detach().max(axis=1)[0].unsqueeze(1)\n",
    "        y_j = rewards + self.gamma * q_target * (1 - dones)          # target, if terminal then y_j = rewards\n",
    "        y_j2 = rewards + self.gamma * q_self * (1 - dones)         \n",
    "        q_eval = self.net_eval(states).gather(1, actions)\n",
    "\n",
    "        # loss backprop\n",
    "        # loss = self.criterion(q_eval, y_j)\n",
    "        loss = max(self.criterion(q_eval, y_j),self.criterion(q_eval, y_j2)) # updated loss function\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # soft update target network\n",
    "        self.softUpdate()\n",
    "\n",
    "    def softUpdate(self):\n",
    "        for eval_param, target_param in zip(self.net_eval.parameters(), self.net_target.parameters()):\n",
    "            target_param.data.copy_(self.tau*eval_param.data + (1.0-self.tau)*target_param.data)\n",
    "\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, n_actions, memory_size, batch_size):\n",
    "        self.n_actions = n_actions\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen = memory_size)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93a1e3e9",
   "metadata": {},
   "source": [
    "Traning and Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f40bd905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, n_episodes=2000, max_steps=1000, eps_start=1.0, eps_end=0.1, eps_decay=0.995, target=200, chkpt=False):\n",
    "    score_hist = []\n",
    "    epsilon = eps_start\n",
    "\n",
    "    bar_format = '{l_bar}{bar:10}| {n:4}/{total_fmt} [{elapsed:>7}<{remaining:>7}, {rate_fmt}{postfix}]'\n",
    "    pbar = trange(n_episodes, unit=\"ep\", bar_format=bar_format, ascii=True)\n",
    "    for idx_epi in pbar:\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for idx_step in range(max_steps):\n",
    "            action = agent.getAction(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.save2memory(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        score_hist.append(score)\n",
    "        score_avg = np.mean(score_hist[-100:])\n",
    "        epsilon = max(eps_end, epsilon*eps_decay)\n",
    "\n",
    "        pbar.set_postfix_str(f\"Score: {score: 7.2f}, 100 score avg: {score_avg: 7.2f}\")\n",
    "        pbar.update(0)\n",
    "\n",
    "        if len(score_hist) >= 100:\n",
    "            if score_avg >= target:\n",
    "                break\n",
    "\n",
    "    if (idx_epi+1) < n_episodes:\n",
    "        print(\"\\nTarget Reached!\")\n",
    "    else:\n",
    "        print(\"\\nDone!\")\n",
    "        \n",
    "    if chkpt:\n",
    "        torch.save(agent.net_eval.state_dict(), 'checkpoint.pth')\n",
    "\n",
    "    with open('CDQN_sn_sc_hist.txt', 'w') as file:\n",
    "        for score in score_hist:\n",
    "            file.write(f\"{score}\\n\")\n",
    "\n",
    "    return score_hist\n",
    "\n",
    "#%% Test Lunar Lander\n",
    "def testLander(env, agent, loop=3):\n",
    "    for i in range(loop):\n",
    "        state = env.reset()\n",
    "        for idx_step in range(500):\n",
    "            action = agent.getAction(state, epsilon=0)\n",
    "            env.render()\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "    env.close()\n",
    "    \n",
    "def plotScore(scores):\n",
    "    plt.figure()\n",
    "    plt.plot(scores)\n",
    "    plt.title(\"Score History\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94829961",
   "metadata": {},
   "source": [
    "## 2. Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57a9abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "EPISODES = 2000\n",
    "TARGET_SCORE = 250.     # early training stop at avg score of last 100 episodes\n",
    "GAMMA = 0.99            # discount factor\n",
    "MEMORY_SIZE = 10000     # max memory buffer size\n",
    "LEARN_STEP = 5          # how often to learn\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "SAVE_CHKPT = False      # save trained network .pth file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfd2ffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "agent = DQN(\n",
    "    n_states = num_states,\n",
    "    n_actions = num_actions,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    lr = LR,\n",
    "    gamma = GAMMA,\n",
    "    mem_size = MEMORY_SIZE,\n",
    "    learn_step = LEARN_STEP,\n",
    "    tau = TAU,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02799e76",
   "metadata": {},
   "source": [
    "## 3. Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "407052f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|1         |   34/2000 [  00:07<  07:21,  4.46ep/s, Score:  -88.93, 100 score avg: -185.20]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/srinath/Documents/C-DQN with SN/CDQN_specnorm.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/CDQN_specnorm.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(device) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/CDQN_specnorm.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# torch.cuda.empty_cache()\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/CDQN_specnorm.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDevice: CUDA\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/CDQN_specnorm.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m score_hist \u001b[39m=\u001b[39m train(env, agent, n_episodes\u001b[39m=\u001b[39;49mEPISODES, target\u001b[39m=\u001b[39;49mTARGET_SCORE, chkpt\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/CDQN_specnorm.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m plotScore(score_hist)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/CDQN_specnorm.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(device) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;32m/home/srinath/Documents/C-DQN with SN/CDQN_specnorm.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/CDQN_specnorm.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx_step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_steps):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/CDQN_specnorm.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mgetAction(state, epsilon)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/CDQN_specnorm.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     next_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/CDQN_specnorm.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     agent\u001b[39m.\u001b[39msave2memory(state, action, reward, next_state, done)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srinath/Documents/C-DQN%20with%20SN/CDQN_specnorm.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     state \u001b[39m=\u001b[39m next_state\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/wrappers/time_limit.py:18\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     15\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     19\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/envs/box2d/lunar_lander.py:344\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    333\u001b[0m     p\u001b[39m.\u001b[39mApplyLinearImpulse(\n\u001b[1;32m    334\u001b[0m         (ox \u001b[39m*\u001b[39m SIDE_ENGINE_POWER \u001b[39m*\u001b[39m s_power, oy \u001b[39m*\u001b[39m SIDE_ENGINE_POWER \u001b[39m*\u001b[39m s_power),\n\u001b[1;32m    335\u001b[0m         impulse_pos,\n\u001b[1;32m    336\u001b[0m         \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    337\u001b[0m     )\n\u001b[1;32m    338\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlander\u001b[39m.\u001b[39mApplyLinearImpulse(\n\u001b[1;32m    339\u001b[0m         (\u001b[39m-\u001b[39mox \u001b[39m*\u001b[39m SIDE_ENGINE_POWER \u001b[39m*\u001b[39m s_power, \u001b[39m-\u001b[39moy \u001b[39m*\u001b[39m SIDE_ENGINE_POWER \u001b[39m*\u001b[39m s_power),\n\u001b[1;32m    340\u001b[0m         impulse_pos,\n\u001b[1;32m    341\u001b[0m         \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    342\u001b[0m     )\n\u001b[0;32m--> 344\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mworld\u001b[39m.\u001b[39;49mStep(\u001b[39m1.0\u001b[39;49m \u001b[39m/\u001b[39;49m FPS, \u001b[39m6\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m30\u001b[39;49m, \u001b[39m2\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m30\u001b[39;49m)\n\u001b[1;32m    346\u001b[0m pos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlander\u001b[39m.\u001b[39mposition\n\u001b[1;32m    347\u001b[0m vel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlander\u001b[39m.\u001b[39mlinearVelocity\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/envs/box2d/lunar_lander.py:73\u001b[0m, in \u001b[0;36mContactDetector.BeginContact\u001b[0;34m(self, contact)\u001b[0m\n\u001b[1;32m     70\u001b[0m     contactListener\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[1;32m     71\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39m=\u001b[39m env\n\u001b[0;32m---> 73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mBeginContact\u001b[39m(\u001b[39mself\u001b[39m, contact):\n\u001b[1;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m     75\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mlander \u001b[39m==\u001b[39m contact\u001b[39m.\u001b[39mfixtureA\u001b[39m.\u001b[39mbody\n\u001b[1;32m     76\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mlander \u001b[39m==\u001b[39m contact\u001b[39m.\u001b[39mfixtureB\u001b[39m.\u001b[39mbody\n\u001b[1;32m     77\u001b[0m     ):\n\u001b[1;32m     78\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mgame_over \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if str(device) == \"cuda\":\n",
    "    # torch.cuda.empty_cache()\n",
    "    print(\"Device: CUDA\")\n",
    "score_hist = train(env, agent, n_episodes=EPISODES, target=TARGET_SCORE, chkpt=True)\n",
    "plotScore(score_hist)\n",
    "\n",
    "if str(device) == \"cuda\":\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d78ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(agent.net_eval.state_dict(), 'checkpoint_cdqn_specnorm.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c47764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from checkpoint.pth\n",
    "# agent.net_eval.load_state_dict(torch.load('./checkpoint_jun30_1k_iter.pth'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9cb59b1",
   "metadata": {},
   "source": [
    "## 4. Test the LunarLander!\n",
    "Run code below to test trained result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b48127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testLander(env, agent, loop=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be4be6b2",
   "metadata": {},
   "source": [
    "Run code below to save tested results into a gif!   \n",
    "Saved gifs will appear in `gifs` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03247c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving gif...Done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import imageio\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def TextOnImg(img, score):\n",
    "    img = Image.fromarray(img)\n",
    "    font = ImageFont.truetype('./arial.ttf', 18)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw.text((20, 20), f\"Score={score: .2f}\", font=font, fill=(255, 255, 255))\n",
    "\n",
    "    return np.array(img)\n",
    "\n",
    "def save_frames_as_gif(frames, filename, path=\"gifs/\"):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    print(\"Saving gif...\", end=\"\")\n",
    "    imageio.mimsave(path + filename + \".gif\", frames, duration=60)\n",
    "\n",
    "    print(\"Done!\")\n",
    "\n",
    "def gym2gif(env, agent, filename=\"gym_animation_cdqn_sn\", loop=1):\n",
    "    frames = []\n",
    "    for i in range(loop):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for idx_step in range(500):\n",
    "            frame = env.render(mode=\"rgb_array\")\n",
    "            frames.append(TextOnImg(frame, score))\n",
    "            action = agent.getAction(state, epsilon=0)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "    env.close()\n",
    "    save_frames_as_gif(frames, filename=filename)\n",
    "\n",
    "gym2gif(env, agent, loop=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550880bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "01c36111a1525a5ff407868170e7863364c687246ea4e433c1c4d9e429760133"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
